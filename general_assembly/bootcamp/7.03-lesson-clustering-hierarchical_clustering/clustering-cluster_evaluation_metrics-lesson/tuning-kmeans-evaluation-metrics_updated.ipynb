{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Tuning K-Means and Cluster Evaluation Metrics\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "![](http://scikit-learn.org/stable//_images/sphx_glr_plot_kmeans_silhouette_analysis_003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "- Evaluate the results of a K-Means Clustering Analysis\n",
    "- Understand inertia and how K-Means is fit\n",
    "- See visually why inertia can lead to poor clusters\n",
    "- Understand the importance of visually evaluating clusters\n",
    "- Learn how to compute cohesion, separation, and the silhouette coefficient\n",
    "- Interpret the silhouette coefficient\n",
    "- Visualize the silhouette across different numbers of clusters\n",
    "- Learn how to use the elbow method with inertia to choose K\n",
    "- Try out alternative evaluation metrics that are only available when true labels exist \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Review: K-Means](#review)\n",
    "- [Introduction: problems with K-Means](#intro)\n",
    "- [Fitting K-Means: Inertia](#inertia)\n",
    "- [Evaluating clusters](#evaluating)\n",
    "- [Visual evaluation of clusters](#visual)\n",
    "- [The silhouette score/coefficient](#sil)\n",
    "    - [Cohesion](#cohesion)\n",
    "    - [Separation](#separation)\n",
    "    - [Silhouette coefficient](#sil-coef)\n",
    "- [Interpreting the silhouette coefficient](#interpret-sil)\n",
    "- [Using the silhouette coefficient to choose the optimal K](#optimal-k)\n",
    "- [Explore the silhouette score with chemical composition data](#chem)\n",
    "    - [Sklearn silhouette plotting function](#sklearn-plot)\n",
    "    - [Inertia vs. K clusters: the elbow method](#inertia-plot)\n",
    "- [Evaluating clusters when the ground truth is available](#ground)\n",
    "    - [Completeness score](#completeness)\n",
    "    - [Homogeneity](#homo)\n",
    "    - [V measure score](#v-measure)\n",
    "    - [Mutual information score](#mutual)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='review'></a>\n",
    "## Review: K-Means clustering\n",
    "---\n",
    "\n",
    "### How do we use K-Means end-to-end?\n",
    "- Discuss how we would use this step-by-step.\n",
    "\n",
    "### What are some problems with K-Means?\n",
    "- Discuss some of the shortcomings of K-Means.\n",
    "- How might we think about \"good\" clusters vs. \"bad\" clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction: problems with K-Means\n",
    "\n",
    "---\n",
    "\n",
    "There are many problems with K-Means. One of the biggest is that it's possible to get widely different results based on initialization. Other problems include:\n",
    "- Different solutions can converge with random initialization\n",
    "- Not everything can be reflected as a \"glob\" of points\n",
    "   - Irregular shapes\n",
    "   - Different densities\n",
    "   - Inconsistent cluster spacing\n",
    "- Interpretation of results dificult without subject matter expertise\n",
    "   - Accuracy is highly subjective\n",
    "   - No formula for the correct number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.aishack.in/static/img/tut/kmeans-bad-initial-guess.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inertia'></a>\n",
    "\n",
    "## Fitting K-Means: Inertia\n",
    "\n",
    "---\n",
    "\n",
    "The K-Means fitting procedure is based on \"Inertia\", or the within-cluster sum of squares criterion. Inertia can be recognized as a measure of internal coherence.\n",
    "\n",
    "### $$ \\text{Inertia } = \\sum_{i=1}^n \\underset{\\mu_i \\in C}{min}(||x_j - \\mu_i||^2) $$\n",
    "\n",
    "Where $C$ denotes the set of clusters.\n",
    "\n",
    "**Drawbacks:**\n",
    "- Fitting using Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.\n",
    "- Inertia is not a *normalized* metric: we just know that lower values are better and zero is optimal. In very high-dimensional spaces, Euclidean distances tend to become inflated (this is one instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as PCA prior to k-means clustering can alleviate this problem and speed up the computations.\n",
    "\n",
    "![inertia2](./assets/inertia2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluating'></a>\n",
    "## Evaluating clusters\n",
    "\n",
    "---\n",
    "\n",
    "The best evaluation of your clustering algorithm is the visual inspection of the clusters. However, it is rare that we have a dataset with few enough features that visual inspection is reasonable. We need to be able to compute metrics that measure how good given clusters fit the structure of your data.\n",
    "\n",
    "If we were to make some basic assumptions about cluster quality, they would be:\n",
    "\n",
    "- High intra-class similarity (within clusters)\n",
    "- Low inter-class similarity (betwee clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"visual\"></a>\n",
    "## Visual evaluation of clusters\n",
    "\n",
    "---\n",
    "\n",
    "When evaluating clusters, the best and easiest method when the data allows is to visually examine the output of the clustering algorithm. After we run the algorithm and calculate the centroids we can plot the clusters to see where the centroids are located and how well the clusters group the data. \n",
    "\n",
    "![](http://simplystatistics.org/wp-content/uploads/2014/02/kmeans.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, silhouette_samples, silhouette_score\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some clusters\n",
    "X, y = make_blobs(n_samples=1000,\n",
    "                  n_features=2,\n",
    "                  centers=3,\n",
    "                  cluster_std=2,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=21)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-means\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix permuted labels\n",
    "from scipy import stats\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(4):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = stats.mode(y[mask])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot our centroids\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='rainbow');\n",
    "\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=500, linewidths=5,\n",
    "            color='k', zorder=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sil'></a>\n",
    "## The silhouette score/coefficient \n",
    "\n",
    "---\n",
    "\n",
    "The silhouette coefficient evaluates the validity of your model's clusters based on the ideas of **cohesion** and **separation**. \n",
    "\n",
    "<a id='cohesion'></a>\n",
    "### Cohesion\n",
    "\n",
    "Cohesion measures clustering effectiveness within a cluster. It is calculated:\n",
    "\n",
    "### $$ \\hat{C}(C_i) = \\underset{x \\in C_i}{\\sum} d(x, c_i) $$\n",
    "\n",
    "Where $C_i$ is the $i$th cluster, $c_i$ is the $i$th centroid, $d$ is the distance metric, and $x$ is the observation.\n",
    "\n",
    "<a id='separation'></a>\n",
    "### Separation\n",
    "\n",
    "Separation measures clustering effectiveness *between* clusters. It is calculated:\n",
    "\n",
    "### $$\\hat{S}(C_i, C_j) = d(c_i, c_j) $$\n",
    "\n",
    "Where $C_i$ and $C_j$ are clusters, $d$ is the distance metric, and $c_i$ and $c_j$ are the clusters' centroids.\n",
    "\n",
    "\n",
    "![](https://snag.gy/FLwIql.jpg)\n",
    "\n",
    "<a id='sil-coef'></a>\n",
    "\n",
    "### Silhouette coefficient\n",
    "\n",
    "The silhouette coefficient combines the cohesion and the separation into a single metric. The silhouette coefficient for a single observation $i$ is calculated:\n",
    "\n",
    "### $$ SC_i = \\frac{b_i - a_i}{max(a_i, b_i)} $$\n",
    "\n",
    "such that:\n",
    "- $a_i$ = mean intra-cluster distance of the sample to others within the cluster.\n",
    "- $b_i$ = the mean nearest-cluster distance from the sample to those within the nearest non-assigned cluster.\n",
    "\n",
    "**The coefficient ranges from -1 to 1:**\n",
    "- A score of 1 indicates the maximum cohesion and separation of clusters.\n",
    "- A score of -1 indicates the minimum cohesion and separation of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://snag.gy/3eCGRi.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='interpret-sil'></a>\n",
    "## Interpreting the silhouette coefficient\n",
    "---\n",
    "\n",
    "When you calculate the silhouette score using sklearn you get out a single number. THis is the average silhouette score for all of the individual observations.\n",
    "\n",
    "In general, we want separation to be high and cohesion to be low. This corresponds to a value of **SC** close to +1.\n",
    "\n",
    "A negative silhouette coefficient means the cluster radius is larger than the space between clusters, and thus clusters overlap. Another way to think about this is that negative values indicate that non-assigned clusters are more similar than the assigned cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='optimal-k'></a>\n",
    "## Using the silhouette coefficient to choose the optimal K\n",
    "\n",
    "---\n",
    "\n",
    "We can use the silhouette coefficient to determine an optimal K.  **It is important to keep in mind that this is still a subjective measure and not an official measure of quality.  It can help you but it's no substitute for knowing your data**.  \n",
    "\n",
    "Visually inspecting your data and evaluating how the silhouette changes across numbers of clusters can give you a rough sense of the quality for K clusters in terms in cohesion and seperation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='chem'></a>\n",
    "## Explore the silhouette score with chemical composition data\n",
    "---\n",
    "\n",
    "Let's load in a dataset on chemical composition and plot the silhouette scores for different numbers of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "csv_file = \"https://vincentarelbundock.github.io/Rdatasets/csv/cluster/chorSub.csv\"\n",
    "df = pd.read_csv(csv_file).iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"scatter\", x=\"Ti\", y=\"K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn-plot'></a>\n",
    "### Sklearn silhouette plotting function\n",
    "\n",
    "Below is some code taken from the sklearn website that helps visualize the silhouette score across different numbers of clusters.\n",
    "\n",
    "The vertical line indicates the average silhouette score across all observations. The individual scores are plotted in sorted, colored sections for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "X = df[['Ti', 'K']].values\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        cmap = cm.get_cmap(\"Spectral\")\n",
    "        color = cmap(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhoutte score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    cmap = cm.get_cmap(\"Spectral\")\n",
    "    colors = cmap(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    \n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inertia-plot'></a>\n",
    "### Inertia vs. K clusters: the elbow method\n",
    "\n",
    "You can plot the inertia vs. the K number of clusters to get an idea of what the optimal number of clusters would be for the dataset. The \"elbow\" technique, though controversial, is a great heuristic to evaluate the optimal K.  Basically, we look for the K where the inertia has an \"elbow\": the point where decreases in inertia are considerably more marginal than for previous increases in K.\n",
    "\n",
    ">\"More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified.\" [Elbow Method](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method)\n",
    "\n",
    "An elbow plot may look something like this:\n",
    "\n",
    "![](http://i.stack.imgur.com/BzwBY.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, sometimes there is not a well-defined elbow!\n",
    "inertias = []\n",
    "for n_clusters in range(2,30):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(X)\n",
    "    inertia = kmeans.inertia_\n",
    "    inertias.append(inertia)\n",
    "\n",
    "plt.plot(inertias)\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.title(\"Inertia across K in chemical data\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ground'></a>\n",
    "\n",
    "## Evaluating clusters when the ground truth is available\n",
    "\n",
    "---\n",
    "\n",
    "The silhouette score is a particularly useful metric in that it does not require us to have the true labels for the clusters (which is most often the case if we need to do clustering to begin with!).\n",
    "\n",
    "When the true labels are available, there are other methods we can use to evaluate the performance of our clustering algorithm and choice of K.\n",
    "- Completeness Score \n",
    "- Homogeneity \n",
    "- V Measure Score \n",
    "- Mutual Information Score\n",
    "\n",
    "**Load the wine dataset and pull out red vs. white as the true clusters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('./datasets/winequality_merged.csv')\n",
    "wine.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = wine.red_wine.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the other variables to use for clustering.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = wine.iloc[:, :-1]\n",
    "Xs = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit a KMeans model with K=2 and extract the predicted labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_wine = KMeans(n_clusters=2)\n",
    "k_wine.fit(Xs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = k_wine.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='completeness'></a>\n",
    "### Completeness score\n",
    "\n",
    "**Completeness indicates that all members of a given class are assigned to the same cluster.**\n",
    "- A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster. (If a cluster contains all of the data points of a single class.) \n",
    "- Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n",
    "\n",
    "You can calculate completeness score using sklearn with:\n",
    "```python\n",
    "from sklearn.metrics import completeness_score\n",
    "print completeness_score(true_clusters, predicted_clusters)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import completeness_score\n",
    "completeness_score(true, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='homo'></a>\n",
    "\n",
    "### Homogeneity\n",
    "\n",
    "**Homogeneity indicates each cluster contains only members of a single class.**\n",
    "- A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class. (Every cluster is composed of data points from only 1 class.  Essentually there are representative of a class)\n",
    "- score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
    "\n",
    "You can calculate the homogeneity using sklearn:\n",
    "```python\n",
    "from sklearn.metrics import homogeneity_score\n",
    "print homogeneity_score(true_clusters, predicted_clusters)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import homogeneity_score\n",
    "homogeneity_score(true, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='v-measure'></a>\n",
    "\n",
    "## V measure score\n",
    "\n",
    "The V measure score is a combination of the homogeneity and completeness metrics:\n",
    "\n",
    "### $$ V = \\frac{2 \\cdot \\text{homogeneity} \\cdot \\text{completeness}}{\\text{homogeneity} + \\text{completeness}} $$\n",
    "\n",
    "In sklearn:\n",
    "```python\n",
    "from sklearn.metrics import v_measure_score\n",
    "print v_measure_score(true_clusters, predicted_clusters)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import v_measure_score\n",
    "v_measure_score(true, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mutual'></a>\n",
    "\n",
    "### Mutual information score\n",
    "\n",
    "**Mutual information measures the agreement between two assignments (clusters, in this case).**\n",
    "- There are various mutual information scores in Sklearn. Be wary of the basic `mutual_info_score` as the output does not have a normal scale, thus making it much harder to judge.\n",
    "\n",
    "### $$ MI(i,j) = \\sum_{a,b} P(a_i, b_j) \\cdot log\\left(\\frac{P(a_i, b_j)}{P(a_i) \\cdot P(b_j)}\\right) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$MI(i,j)$ is the mutual information of assignment $i$ and $j$, which can be predicted vs. true.\n",
    "\n",
    "$P(a_i, a_j)$ is the probability that observation $a$ in group $i$ is the same as observation $b$ in group $j$. In other words, the probability that the predicted and true are the same.\n",
    "\n",
    "$P(a_i)$ is the probability of observation $a$ in group $i$, and $P(b_i)$ is the probability of observation $b$ in group $j$.\n",
    "\n",
    "The unnormalized mutual information score can be calculated in sklearn with:\n",
    "```python\n",
    "from sklearn.metrics import mutual_info_score\n",
    "mutual_info_score(true_clusters, predicted_clusters)\n",
    "```\n",
    "\n",
    "That is not guaranteed to be between 0. and 1. For the normalized version that is, use the adjusted scorer:\n",
    "```python\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "adjusted_mutual_info_score(true_clusters, predicted_clusters)\n",
    "```\n",
    "\n",
    "**Adjusted mutual information:**\n",
    "\n",
    "### $$ AMI(U, V) = \\frac{MI(U, V) - E(MI(U, V))}{max(H(U), H(V)) - E(MI(U, V))} $$\n",
    "\n",
    "Adjusted accounts for chance or variance.  A Standard MI Score tends to favor comparing cluster labels when there are a large amount of clusters.  This is nulled when using AMI.\n",
    "\n",
    "AMI is also on a normalized scale of 0 to 1.0 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score, adjusted_mutual_info_score\n",
    "\n",
    "print(mutual_info_score(true, predicted))\n",
    "print(adjusted_mutual_info_score(true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- [Davies Bouldin Index (DBI)](https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index) _- The Davies-Bouldin index is the sum of the worst intra-to-inter cluster distance ratios over all kk clusters_\n",
    "- [Dunn index](https://en.wikipedia.org/wiki/Dunn_index) _- Ratio between the smallest distance between observations not in the same cluster to the largest intra-cluster distance_\n",
    "- [Clustering Countries Real GDP Growth](http://www.turingfinance.com/clustering-countries-real-gdp-growth-part2/)\n",
    "- [SKLearn Clustering](http://scikit-learn.org/stable/modules/clustering.html)\n",
    "- [SKLearn K-Means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "- [SKLearn Silhouette Score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
