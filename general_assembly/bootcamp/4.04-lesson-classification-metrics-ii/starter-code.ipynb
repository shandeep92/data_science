{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Classification Metrics II\n",
    "\n",
    "Author: Matt Brems (DC), Dave Yerrington (SF)\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Describe the inverse relationship between sensitivity and specificity.\n",
    "- Construct the ROC curve.\n",
    "- Understand how ROC AUC is calculated and interpret ROC AUC.\n",
    "- Identify methods for handling unbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data.\n",
    "df = pd.read_csv('./data/Whickham.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out first five rows.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `outcome`: Whether someone is alive or dead.\n",
    "- `smoker`: Whether somebody smoked or did not smoke.\n",
    "- `age`: Age in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outcome_dummy column with value 1 if \n",
    "# dead and 0 if alive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of our outcome_dummy variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm we created outcome_dummy properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create smoker_dummy column with value 1 if\n",
    "# smoker and 0 if non-smoker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the interesting things about this data is the relationship between age and smoking.\n",
    "- You can read more about it [here](https://www2.stat.duke.edu/courses/Spring08/sta103/whickham.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we usually account for a relationship\n",
    "# between two independent variables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and y.\n",
    "\n",
    "X =\n",
    "y ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Conduct a train/test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Logistic Regression model.\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model as lr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read more about the solver [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View coefficients.\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret coefficient for age.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How would you interpret the coefficient for age?</summary>\n",
    "\n",
    "- As age increases by 1, someone is 1.134 times as likely to be dead.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>In this situation, what term would we use to describe someone who is predicted to be dead but actually is alive? (Remember that alive is coded as 0 and dead is coded as 1.)</summary>\n",
    "\n",
    "- We **falsely** predict someone to be **positive**.\n",
    "- This would be a **false positive**.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>In this situation, what is a true negative?</summary>\n",
    "\n",
    "- We **correctly** predict someone to be **negative**.\n",
    "- Someone who is predicted to be alive (`0`) and actually is alive (`0`).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion matrix.\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If I want to get a good understanding of how our model will do on new data, should I generate a confusion matrix on our training or testing set? Why?</summary>\n",
    "    \n",
    "- Testing set!\n",
    "- If we generate one on our training set, we're going to overestimate the performance of our model... just like if we calculated MSE on our training set.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "preds ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix.\n",
    "# Documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "confusion_matrix(y_test, # True values.\n",
    "                 preds)  # Predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TN/FP/FN/TP values.\n",
    "\n",
    "tn, fp, fn, tp =\n",
    "\n",
    "# Note that .ravel() will arrange items in a one-dimensional array.\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the test specificity of our model?\n",
    "\n",
    "spec =\n",
    "\n",
    "print(f'Specificity: {round(spec,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the test sensitivity of our model?\n",
    "\n",
    "sens =\n",
    "\n",
    "print(f'Sensitivity: {round(sens,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between Sensitivity and Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dataframe called pred_df that contains:\n",
    "# 1. The list of true values of our test set.\n",
    "# 2. The list of predicted probabilities based on our model.\n",
    "\n",
    "pred_proba =\n",
    "\n",
    "pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                        'pred_probs':pred_proba})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create histogram of observations with 25 bins.\n",
    "\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create two histograms of observations.\n",
    "plt.hist(pred_df[pred_df['true_values'] == 0]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='b',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 0')\n",
    "plt.hist(pred_df[pred_df['true_values'] == 1]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='orange',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 1')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What do you notice about this distribution?</summary>\n",
    "\n",
    "- Way more blue than orange.\n",
    "- There's lots of overlap!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create two histograms of observations.\n",
    "plt.hist(pred_df[pred_df['true_values'] == 0]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='b',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 0')\n",
    "plt.hist(pred_df[pred_df['true_values'] == 1]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='orange',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 1')\n",
    "\n",
    "# Add vertical line at P(Outcome = 1) = 0.5.\n",
    "plt.vlines(x=0.5,\n",
    "           ymin = 0,\n",
    "           ymax = 65,\n",
    "           color='r',\n",
    "           linestyle = '--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Where are my true positives, true negatives, false positives, false negatives in this image?</summary>\n",
    "    \n",
    "- True Positive:\n",
    "    - Items I correctly predict to be positive.\n",
    "    - The orange bars (actual `1`) that are to the right of the red line (predicted `1`).\n",
    "- True Negative: \n",
    "    - Items I correctly predict to be negative.\n",
    "    - The blue bars (actual `0`) that are to the left of the red line (predicted `0`).\n",
    "- False Positive:\n",
    "    - Items I incorrectly predict to be positive.\n",
    "    - The blue bars (actual `0`) that are to the right of the red line (predicted `1`).\n",
    "- False Negative: \n",
    "    - Items I incorrectly predict to be negative.\n",
    "    - The orange bars (actual `1`) that are to the left of the red line (predicted `0`).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create two histograms of observations.\n",
    "plt.hist(pred_df[pred_df['true_values'] == 0]['pred_probs'],\n",
    "         bins = 25,\n",
    "         color = 'b',\n",
    "         alpha = 0.6,\n",
    "         label = 'Outcome = 0')\n",
    "plt.hist(pred_df[pred_df['true_values'] == 1]['pred_probs'],\n",
    "         bins = 25,\n",
    "         color = 'orange',\n",
    "         alpha = 0.6,\n",
    "         label = 'Outcome = 1')\n",
    "\n",
    "# Add vertical line at P(Outcome = 1) = 0.5.\n",
    "plt.vlines(x = 0.5,\n",
    "           ymin = 0,\n",
    "           ymax = 65,\n",
    "           color = 'r',\n",
    "           linestyle = '--')\n",
    "\n",
    "# Add annotations for TN, FN, TP, FP.\n",
    "plt.annotate(xy = (0.04, 15),\n",
    "             s = 'TN',\n",
    "             size = 20)\n",
    "\n",
    "plt.annotate(xy = (0.1, 0),\n",
    "             s = 'FN',\n",
    "             size = 20)\n",
    "\n",
    "plt.annotate(xy = (0.85, 1),\n",
    "             s = 'TP',\n",
    "             size = 20)\n",
    "\n",
    "plt.annotate(xy = (0.53, 1),\n",
    "             s = 'FP',\n",
    "             size = 20)\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize = 22)\n",
    "plt.ylabel('Frequency', fontsize = 18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize = 18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's adjust our classification threshold to be lower. Instead of using 50% as the cutoff between the positive and negative classes, let's move that threshold down to 10%.\n",
    "- Any observation with a **predicted probability above 10%** would be **predicted to be in the positive class**.\n",
    "- Any observation with a **predicted probability below 10%** would be **predicted to be in the negative class**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create two histograms of observations.\n",
    "plt.hist(pred_df[pred_df['true_values'] == 0]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='b',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 0')\n",
    "plt.hist(pred_df[pred_df['true_values'] == 1]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='orange',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 1')\n",
    "\n",
    "# Add vertical line at P(Outcome = 1) = 0.1.\n",
    "plt.vlines(x=0.1,\n",
    "           ymin = 0,\n",
    "           ymax = 65,\n",
    "           color='r',\n",
    "           linestyle = '--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>When I moved my classification threshold to the left, what happened to sensitivity and specificity?</summary>\n",
    "\n",
    "- Our number of true negatives decreased and our number of total negatives remains the same.\n",
    "    - $\\text{Specificity} = \\frac{TN}{N} \\Rightarrow \\text{Specificity decreases.}$\n",
    "- Our number of true positives increased and our number of total positives remains the same.\n",
    "    - $\\text{Sensitivity} = \\frac{TP}{P} \\Rightarrow \\text{Sensitivity increases.}$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal should be to build a model such that there is **no overlap** between the blue histogram and the orange histogram!\n",
    "- If there is overlap, we need to recognize the tradeoff between sensitivity and specificity. (As one increases, the other decreases.)\n",
    "- One measure of how much overlap exists between our distributions is the **area under the ROC curve**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "The Receiver Operating Characteristic curve is a way to visualize the overlap between our positive class and negative class by moving our classification threshold from 0 to 1.\n",
    "- We start our classification threshold (dashed red line) at 0.\n",
    "- We calculate sensitivity and 1 - specificity.\n",
    "- We plot the value of (y = sensitivity, x = 1 - specificity).\n",
    "- We increase our classification threshold a small number (like 0.005).\n",
    "- We calculate sensitivity and 1 - specificity.\n",
    "- We plot the value of (y = sensitivity, x = 1 - specificity).\n",
    "- We repeat until our threshold is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create threshold values.\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "\n",
    "# Define function to calculate sensitivity. (True positive rate.)\n",
    "def TPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_positive = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "    false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "    \n",
    "\n",
    "# Define function to calculate 1 - specificity. (False positive rate.)\n",
    "def FPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_negative = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "    false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "    return 1 - (true_negative / (true_negative + false_positive))\n",
    "    \n",
    "# Calculate sensitivity & 1-specificity for each threshold between 0 and 1.\n",
    "tpr_values = [TPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "fpr_values = [FPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "\n",
    "# Plot ROC curve.\n",
    "plt.plot(fpr_values, # False Positive Rate on X-axis\n",
    "         tpr_values, # True Positive Rate on Y-axis\n",
    "         label='ROC Curve')\n",
    "\n",
    "# Plot baseline. (Perfect overlap between the two populations.)\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Receiver Operating Characteristic Curve', fontsize=22)\n",
    "plt.ylabel('Sensitivity', fontsize=18)\n",
    "plt.xlabel('1 - Specificity', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC\n",
    "\n",
    "The more area under this blue curve is, the better separated our distributions are.\n",
    "- Check out [this gif](https://twitter.com/DrHughHarvey/status/1104435699095404544).\n",
    "\n",
    "We use the **area under the ROC curve** (abbreviated **ROC AUC** or **AUC ROC**) to quantify the gap between our distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Note: Not to be confused with the ROC AOC.</summary>\n",
    "<img src=\"./images/AOC.jpg\" alt=\"AOC\" width=\"400\"/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import roc_auc_score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create threshold values. (Dashed red line in image.)\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "\n",
    "# Define function to calculate sensitivity. (True positive rate.)\n",
    "def TPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_positive = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "    false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "    \n",
    "\n",
    "# Define function to calculate 1 - specificity. (False positive rate.)\n",
    "def FPR(df, true_col, pred_prob_col, threshold):\n",
    "    true_negative = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "    false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "    return 1 - (true_negative / (true_negative + false_positive))\n",
    "    \n",
    "# Calculate sensitivity & 1-specificity for each threshold between 0 and 1.\n",
    "tpr_values = [TPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "fpr_values = [FPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "\n",
    "# Plot ROC curve.\n",
    "plt.plot(fpr_values, # False Positive Rate on X-axis\n",
    "         tpr_values, # True Positive Rate on Y-axis\n",
    "         label='ROC Curve')\n",
    "\n",
    "# Plot baseline. (Perfect overlap between the two populations.)\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')\n",
    "\n",
    "# Label axes.\n",
    "plt.title(f'ROC Curve with AUC = {round(roc_auc_score(pred_df[\"true_values\"], pred_df[\"pred_probs\"]),3)}', fontsize=22)\n",
    "plt.ylabel('Sensitivity', fontsize=18)\n",
    "plt.xlabel('1 - Specificity', fontsize=18)\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting ROC AUC\n",
    "- If you have an ROC AUC of 0.5, your positive and negative populations perfectly overlap and your model is as bad as it can get.\n",
    "- If you have an ROC AUC of 1, your positive and negative populations are perfectly separated and your model is as good as it can get.\n",
    "- The closer your ROC AUC is to 1, the better. (1 is the maximum score.)\n",
    "- If you have an ROC AUC of below 0.5, your positive and negative distributions have flipped sides. By flipping your predicted values (i.e. flipping predicted 1s and 0s), your ROC AUC will now be above 0.5.\n",
    "    - Example: You have an ROC AUC of 0.2. If you change your predicted 1s to 0s and your predicted 0s to 1s, your ROC AUC will now be 0.8!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate one ROC curve per model. The ROC curve is generated by varying our threshold from 0 to 1. This doesn't actually change the threshold or our original predictions, but it helps us to visualize our tradeoff between _sensitivity_ and _specificity_ and understand how well-separated our populations are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced / Unbalanced Classes\n",
    "\n",
    "Suppose I want to predict the incidence of West Nile Virus (WNV) in Chicago.\n",
    "- 99.9% of my observations are \"no WNV.\"\n",
    "- 0.1% of my observations contain \"WNV.\"\n",
    "\n",
    "If we fit a model and tried to optimize for accuracy, I can predict \"no WNV\" for every location and have an accuracy score that is really, _really_ good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why is this a bad model?</summary>\n",
    "    \n",
    "- We'll never predict that a location has West Nile Virus, which is probably going to eventually lead to outbreaks of the disease.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification problems, methods generally perform better when we have roughly equally-sized classes. (i.e. 50% in the positive class and 50% in the negative class for binary classification problems.)\n",
    "\n",
    "#### Methods for Dealing with Unbalanced Classes\n",
    "\n",
    "- **Weighting observations.** Some models allow the weighting of classes such as linear and logistic regression, Naive Bayes, Random Forests, SVMs, etc.\n",
    "\n",
    "<img src=\"./images/class_weight.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "- **Stratified cross-validation.** If we use $k$-fold cross-validation entirely randomly, we may run into issues where some of our folds have no observations from the minority class.\n",
    "\n",
    "<img src=\"https://snag.gy/PqISr3.jpg\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "By stratifying on our output variable with unbalanced classes during cross-validation, we protect ourselves from this situation and ensure that our estimate of our model performance has lower variance.\n",
    "\n",
    "- **Changing threshold for classification.**\n",
    "By adjusting our classification threshold, we might find a better fit for our particular use-case.  We performed this operation earlier by manually changing our predicted labels by choosing a new threshold from the predicted probability.\n",
    "\n",
    "- **Bias correction.** Gary King wrote a [great whitepaper](https://gking.harvard.edu/files/gking/files/0s.pdf) on this topic.  This is a rigorous approach and while provide good results, as data scientists we often prefer \"easier\" methods to implement.\n",
    "\n",
    "- **Purposefully optimizing evaluation metrics.**\n",
    "We might also consider optimizing our model for a specific metric such as precision, recall by class, ROC AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How is the ROC curve calculated?</summary>\n",
    "    \n",
    "- The ROC curve is generated by starting our classification threshold at 0, calculating sensitivity and 1-specificity, and plotting those numbers. We then increment our threshold by a small number (like 0.1%), and calculate and plot sensitivity and 1-specificity again. We repeat this process until our classification threshold has been moved to 1.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Let's say you were building a search engine and wanted to build a classification model that would recommend articles based on the search input. What metric would you want to optimize for and why?</summary>\n",
    "    \n",
    "- You could make a case for wanting to minimize false positives (stories that weren't relevant), in which case you'd want to optimize for precision.\n",
    "- You could make a case for wanting to minimize false negatives (not passing along possibly useful content), in which case you'd want to optimize for recall. \n",
    "- Alumni Comment: \"The interviewer seemed more interested in seeing if I knew what the metrics were and explaining what priorities would lead me to optimize for one over the other.\"\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
